# -*- coding: utf-8 -*-
"""B19EE039_lab-9

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kRvbC7izjV-wEh39NEdOtArT_Z_rxJeD
"""

import pandas  as pd
import numpy as np 
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from numpy import arange
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_openml
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.multiclass import OneVsOneClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.decomposition import PCA

"""# MNIST data"""

X_mn, y_mn = fetch_openml('mnist_784', version=1, return_X_y=True)

X_mn.shape

df_mn = pd.DataFrame(X_mn, columns = [i for i in range (784)])

y_mn

y_mn = y_mn.astype(int)

y_mn

df_mn['target']=y_mn

df_mn.shape

df_mn.head()

df_mn.drop( df_mn[ df_mn['target']>4].index , inplace=True)

df_mn.shape

y_mn_n = df_mn['target']

x_mn_n = df_mn.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(x_mn_n, y_mn_n, test_size=0.1, random_state=1)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22222, random_state=1) 
# 0.9*x=0.2

X_train.shape

X_train.iloc[1000]

import matplotlib
some_digit = X_train.iloc[1040]
some_digit_image = some_digit.values.reshape(28,28)
plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,interpolation='nearest')
plt.axis=('off')

"""# For KNN """

clf_k =  KNeighborsClassifier(n_neighbors= 20)

clf_k.fit(X_train, y_train)

# parameters = {'n_neighbors':[15,20]}

# clf_k_fin = GridSearchCV(clf_k, parameters)

# clf_k_fin.fit(X_train, y_train)

# clf_k_fin.get_params()

accu_k_val= clf_k.score(X_val, y_val)

accu_k = clf_k.score(X_test, y_test)

# accu_k = clf_k_fin.score(X_test)

accu_k

"""# Using mlp"""

clf_mlp = MLPClassifier(max_iter=100)

clf_mlp.fit(X_train, y_train)

# parameters = {'learning_rate':[0.001, 0.01, 0.1], 'max_iter':[200,300,400]}

# clf_mlp_fin= GridSearchCV(clf_mlp, parameters)

# clf_mlp_fin.fit(X_train, y_train)

# clf_mlp_fin.get_params()

# accu_mlp = clf_mlp_fin.score(X_test)

accu_mlp = clf_mlp.score(X_test, y_test)

accu_mlp

"""# Using SVM"""

clf_svm = svm.SVC(C=5)

clf_svm.fit(X_train, y_train)

# parameters = {'C':[1,2,4,8,10]}

# clf_svm_fin= GridSearchCV(clf_svm, parameters)

# clf_svm_fin.fit(X_train, y_train)

# clf_svm_fin.get_params()

# accu_svm = clf_svm_fin.score(X_test)

accu_svm = clf_svm.score(X_test, y_test)

accu_svm

"""# Normalised data"""

scaler = StandardScaler()
scaler.fit(X_train)
X_train= scaler.transform(X_train)
scaler.fit(X_val)
X_val= scaler.transform(X_val)
scaler.fit(X_test)
X_test = scaler.transform(X_test)

# knn
clf_k_nor =  KNeighborsClassifier(n_neighbors= 20)
# parameters = {'n_neighbors':[i for i in range (15,20)]}
# clf_k_fin_nor = GridSearchCV(clf_k_nor, parameters)
clf_k_nor.fit(X_train, y_train)
# clf_k_fin_nor.best_estimator

# clf_k_fin_nor.get_params()

accu_k_nor = clf_k_nor.score(X_test, y_test)

accu_k_nor

# mlp
clf_mlp_nor = MLPClassifier(max_iter= 100)
# parameters = {'learning_rate':[0.001, 0.01, 0.1], 'max_iter':[200,300,400]}
# clf_mlp_fin_nor= GridSearchCV(clf_mlp_nor, parameters)
clf_mlp_nor.fit(X_train, y_train)
# clf_mlp_fin_nor.get_params()

accu_mlp_nor = clf_mlp_nor.score(X_test, y_test)

accu_mlp_nor

# svm
clf_svm_nor = svm.SVC(C=5)
# parameters = {'C':[1,2,4,8,10]}
# clf_svm_fin_nor= GridSearchCV(clf_svm_nor, parameters)
clf_svm_nor.fit(X_train, y_train)
# clf_svm_fin_nor.get_params()

accu_svm_nor = clf_svm_nor.score(X_test, y_test)

accu_svm_nor

"""# Comparison"""

barWidth = 0.25
fig = plt.subplots(figsize =(12, 8))
 
# set height of bar
Not_normalised = [accu_k, accu_mlp, accu_svm]
Normalised = [accu_k_nor,accu_mlp_nor,accu_svm_nor]
 
# Set position of bar on X axis
br1 = np.arange(len(Not_normalised))
br2 = [x + barWidth for x in br1]

 
# Make the plot
plt.bar(br1, Not_normalised, color ='b', width = barWidth, edgecolor ='grey', label ='Not_normalised')
plt.bar(br2, Normalised, color ='r', width = barWidth, edgecolor ='grey', label ='Normalised')

# Adding Xticks
plt.xlabel('Models', fontweight ='bold', fontsize = 15)
plt.ylabel('Accuracy', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(Not_normalised))],['KNN', 'MLP', 'SVM'])
 
plt.legend()
plt.show()

"""# OVO and OVA"""

clf_ova_svm = OneVsRestClassifier(svm.SVC()).fit(X_train, y_train)

accu_ova_svm = clf_ova_svm.score(X_test, y_test)

accu_ova_svm

clf_ovo_svm = clf = OneVsOneClassifier(svm.SVC()).fit(X_train, y_train)

accu_ovo_svm = clf_ovo_svm.score(X_test, y_test)

accu_ovo_svm

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
Classifiers = ['OVA', 'OVO']
students = [23,17,35,29,12]
ax.bar(langs,students)
plt.show()

"""# Diabetes"""

diab_data = pd.read_csv("/content/diabetes.csv")

diab_data.shape

diab_data.head()

y= diab_data["Outcome"]

X= diab_data.drop('Outcome', axis=1)

scaler = StandardScaler()

scaler.fit(X)

X= scaler.transform(X)

X

X.shape

type(X)

X = pd.DataFrame(X, columns = ['Pregnancies',	'Glucose','BloodPressure',	'SkinThickness',	'Insulin',	'BMI',	'DiabetesPedigreeFunction', 	'Age'])

X.head()

y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22222, random_state=1) 
# 0.9*x=0.2

X_train

y_train

y_val

y_test

"""# Using linear kernel"""

# Linear kernel
clf = svm.SVC(kernel='linear')

clf.fit(X_train, y_train)

# Accuracy On validation set
clf.score(X_val, y_val)

clf.score(X_test, y_test)

c_values = np.linspace(0.01, 10, 10)
c_values

scores = [cross_val_score(clf.set_params(C=c_value),X_train,y_train,cv=5) for c_value in c_values]

means = np.array([np.mean(score) for score in scores])
deviations = np.array([np.std(score) for score in scores])

sns.set_style("whitegrid")
figure = plt.figure()
plt.plot(c_values, means,marker='x')
plt.xlabel('c_values')
plt.ylabel('mean_accuracy')

"""#using polynomial kernel

"""

clf_poly = svm.SVC(kernel='poly')

clf_poly.fit(X_train , y_train)

clf_poly.score(X_val, y_val)

clf_poly.score(X_test, y_test)

scores = [cross_val_score(clf_poly.set_params(C=c_value),X_train,y_train,cv=5) for c_value in c_values]

means = np.array([np.mean(score) for score in scores])
deviations = np.array([np.std(score) for score in scores])

sns.set_style("whitegrid")
figure = plt.figure()
plt.plot(c_values, means,marker='x')
plt.xlabel('c_values')
plt.ylabel('mean_accuracy')

"""# Gaussian Kernel

"""

clf_gauss = svm.SVC(kernel='rbf')

clf_gauss.fit(X_train , y_train)

clf_gauss.score(X_val, y_val)

clf_gauss.score(X_test, y_test)

scores = [cross_val_score(clf_gauss.set_params(C=c_value),X_train,y_train,cv=5) for c_value in c_values]

means = np.array([np.mean(score) for score in scores])
deviations = np.array([np.std(score) for score in scores])

sns.set_style("whitegrid")
figure = plt.figure()
plt.plot(c_values, means,marker='x')
plt.xlabel('c_values')
plt.ylabel('mean_accuracy')

"""# Support vectors"""

clf_gauss_fin = svm.SVC(kernel='rbf', C= 2.23)

clf_gauss_fin.fit(X_train , y_train)

clf_gauss_fin.n_support_

clf_poly_fin = svm.SVC(kernel='poly', C=1.12 )

clf_poly_fin.fit(X_train , y_train)

clf_poly_fin.n_support_

clf_lin_fin = svm.SVC(kernel= 'linear', C=0.01)

clf_lin_fin.fit(X_train , y_train)

clf_lin_fin.n_support_

"""# Plots(Applying PCA)"""

pca = PCA(n_components=2)

pca.fit(X)

X= pca.transform(X)

X.shape

df_pca = pd.DataFrame(X, columns = ['PCA1','PCA2'])

df_pca.head()

df_pca['outcome']=y

df_pca.head()

clf_gauss_pca= svm.SVC(kernel='rbf', C= 2.23)

y_train_pca = df_pca['outcome']

x_train_pca = df_pca.drop('outcome', axis=1)

clf_gauss_pca.fit(x_train_pca, y_train_pca)

# Generate a set of points that form a grid over feature space
x1s = np.linspace(min(df_pca.iloc[:, 0]), max(df_pca.iloc[:, 0]), 600)
x2s = np.linspace(min(df_pca.iloc[:, 1]), max(df_pca.iloc[:, 1]), 600)
points = np.array([[x1, x2] for x1 in x1s for x2 in x2s])

# Compute decision function for each point, keep those which are close to the boundary
dist_bias = clf_gauss_pca.decision_function(points)
bounds_bias = np.array([pt for pt, dist in zip(points, dist_bias) if abs(dist)<0.05])

fig, ax1 = plt.subplots(figsize=[20, 10], nrows=1)

ax1.scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], color=["r" if y == 0 else "b" for y in y_train_pca], label="data")
ax1.scatter(bounds_bias[:, 0], bounds_bias[:, 1], color="g", s=3, label="decision boundary")
ax1.set_title("Gaussian kernel")

clf_lin_pca = svm.SVC(kernel= 'linear', C=0.01)

clf_lin_pca.fit(x_train_pca, y_train_pca)

dist_bias = clf_lin_pca.decision_function(points)
bounds_bias = np.array([pt for pt, dist in zip(points, dist_bias) if abs(dist)<0.05])

fig, ax1 = plt.subplots(figsize=[20, 10], nrows=1)

ax1.scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], color=["r" if y == 0 else "b" for y in y_train_pca], label="data")
ax1.scatter(bounds_bias[:, 0], bounds_bias[:, 1], color="g", s=3, label="decision boundary")
ax1.set_title("Linear kernel")

clf_poly_pca = clf_lin_pca = svm.SVC(kernel= 'poly',C=1.12)

clf_poly_pca.fit(x_train_pca, y_train_pca)

dist_bias = clf_poly_pca.decision_function(points)
bounds_bias = np.array([pt for pt, dist in zip(points, dist_bias) if abs(dist)<0.05])

fig, ax1 = plt.subplots(figsize=[20, 10], nrows=1)

ax1.scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], color=["r" if y == 0 else "b" for y in y_train_pca], label="data")
ax1.scatter(bounds_bias[:, 0], bounds_bias[:, 1], color="g", s=3, label="decision boundary")
ax1.set_title("Polynomial kernel")

